{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK for Paragraph split.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5uqeIDEdqEf_"
      },
      "outputs": [],
      "source": [
        "from nltk import tokenize\n",
        "import math\n",
        "import copy\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains a function \"extract_plot_sections\" that takes in a large string (that is, a movie plot description) and outputs 5 different divisions of that string by sentence. It does this by approximating what the \"half\" or a \"third\" of the sentences are (for the exact approximation, see the math.floor function in the method). Make sure to import the proper packages before running the method. All other cells are just for testing. --Ethan"
      ],
      "metadata": {
        "id": "G5FzBPyLQ1ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe2soNALqq3c",
        "outputId": "a65f35f8-da90-4a14-b9bc-49130c8b4c53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = 'Good morning Dr. Adams. The patient is waiting for you in room 2. Have you seen the chart? It looks like the flu so I prescribed Ibuprofen.'\n",
        "\n",
        "sentences = tokenize.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "P__5iA_7qVA4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_plot_sections(paragraphs_string):\n",
        "  # from nltk import tokenize\n",
        "  # make sure to run nltk.download('punkt') \n",
        "  # import math\n",
        "  # import copy\n",
        "  sentences = tokenize.sent_tokenize(paragraph) # this returns a list of sentences, split by an NLP library\n",
        "  num_sentences = len(sentences)\n",
        "  \n",
        "  thirds = math.floor(num_sentences/3.0)\n",
        "  half = math.floor(num_sentences/2.0)\n",
        "\n",
        "  first_third = copy.copy(sentences[:thirds])\n",
        "  first_third_string = \" \".join(first_third)\n",
        "  \n",
        "  second_third = copy.copy(sentences[thirds:2*thirds])\n",
        "  second_third_string = \" \".join(second_third)\n",
        "\n",
        "  last_third = copy.copy(sentences[2*thirds:])\n",
        "  last_third_string = \" \".join(last_third)\n",
        "\n",
        "  first_half = copy.copy(sentences[:half])\n",
        "  first_half_string = \" \".join(first_half)\n",
        "\n",
        "  second_half = copy.copy(sentences[half:])\n",
        "  second_half_string = \" \".join(second_half)\n",
        "\n",
        "  return first_third_string, second_third_string, last_third_string, first_half_string, second_half_string"
      ],
      "metadata": {
        "id": "rdrhN71GqXQv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_plot_sections(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9u8nis8un89",
        "outputId": "83460376-40fc-46cd-d6ca-163eed91667f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Good morning Dr. Adams.',\n",
              " 'The patient is waiting for you in room 2.',\n",
              " 'Have you seen the chart? It looks like the flu so I prescribed Ibuprofen.',\n",
              " 'Good morning Dr. Adams. The patient is waiting for you in room 2.',\n",
              " 'Have you seen the chart? It looks like the flu so I prescribed Ibuprofen.')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lJa-6v-IusNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}